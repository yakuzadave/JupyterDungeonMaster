{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c077038dfcc4d67bb70d2162dd2d339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c911fac374947238de9f647a2e9244f",
              "IPY_MODEL_acb817ca80ec45fda84bd0ca70ca5e68",
              "IPY_MODEL_8bd874f7cb2e45d9a2bb7e25642cb46b"
            ],
            "layout": "IPY_MODEL_8055bc7170314316a2d1bedf82595660"
          }
        },
        "8c911fac374947238de9f647a2e9244f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d18a5ea443a64825b4367ff38f96d442",
            "placeholder": "​",
            "style": "IPY_MODEL_a38c4a01a17e4f36a02fe3f679b53ab3",
            "value": "100%"
          }
        },
        "acb817ca80ec45fda84bd0ca70ca5e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_924dcaf797c8442f9d582428893b738d",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be66921235fe48f7a245837b4fc1652b",
            "value": 4
          }
        },
        "8bd874f7cb2e45d9a2bb7e25642cb46b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eb32f9e9ad04b9da93445d864bb9e01",
            "placeholder": "​",
            "style": "IPY_MODEL_99511ff3cee047018fc1b2dd7f7921d3",
            "value": " 4/4 [00:00&lt;00:00, 107.28it/s]"
          }
        },
        "8055bc7170314316a2d1bedf82595660": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d18a5ea443a64825b4367ff38f96d442": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a38c4a01a17e4f36a02fe3f679b53ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "924dcaf797c8442f9d582428893b738d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be66921235fe48f7a245837b4fc1652b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4eb32f9e9ad04b9da93445d864bb9e01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99511ff3cee047018fc1b2dd7f7921d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dad8dc686d0b4a70b296290bcee2c11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_496df0f2c59648688af76c81b1e08594",
              "IPY_MODEL_04dc5a4e611843f6bf381546d168beea",
              "IPY_MODEL_57ec014c3f4c4cb18e919d8d10dc64ef"
            ],
            "layout": "IPY_MODEL_73524d19561843b1a26ed9cdabb7068a"
          }
        },
        "496df0f2c59648688af76c81b1e08594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69f1d6da3184479681af47b70ef48cfe",
            "placeholder": "​",
            "style": "IPY_MODEL_f897de046c8d4dda808df2f7f437df96",
            "value": "100%"
          }
        },
        "04dc5a4e611843f6bf381546d168beea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_915cd7a2be394c16b661e9652b7e55a9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e40d00a5b3cf4d7d86f5f78cd11aa73d",
            "value": 1
          }
        },
        "57ec014c3f4c4cb18e919d8d10dc64ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f753e3b43db44986bb344bd5eadce65a",
            "placeholder": "​",
            "style": "IPY_MODEL_90290db43cf84dca84fb348d6c7fb213",
            "value": " 1/1 [00:02&lt;00:00,  2.81s/it]"
          }
        },
        "73524d19561843b1a26ed9cdabb7068a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69f1d6da3184479681af47b70ef48cfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f897de046c8d4dda808df2f7f437df96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "915cd7a2be394c16b661e9652b7e55a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e40d00a5b3cf4d7d86f5f78cd11aa73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f753e3b43db44986bb344bd5eadce65a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90290db43cf84dca84fb348d6c7fb213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yakuzadave/JupyterDungeonMaster/blob/main/LangChain_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain: Tools & Chains"
      ],
      "metadata": {
        "id": "XoJ-RGiUo-uJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RSTxnoIozRN",
        "outputId": "5da045a7-a568-4ebd-e0ba-e15f549f4eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.7 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install tiktoken pinecone-client pinecone-text weaviate-client openai langchain huggingface_hub d20 pinecone-client google-search-results gspread PyYAML python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml\n",
        "!mv openapi.yaml openai_openapi.yaml\n",
        "!wget https://www.klarna.com/us/shopping/public/openai/v0/api-docs\n",
        "!mv api-docs klarna_openapi.yaml\n",
        "!wget https://raw.githubusercontent.com/APIs-guru/openapi-directory/main/APIs/spotify.com/1.0.0/openapi.yaml\n",
        "!mv openapi.yaml spotify_openapi.yaml"
      ],
      "metadata": {
        "id": "ctB9u491Yune"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hwchase17/langchain-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogf0KAghgo78",
        "outputId": "e7b614b2-5fc9-40e6-9643-f2f5a2cea7c2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'langchain-hub'...\n",
            "remote: Enumerating objects: 455, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 455 (delta 28), reused 32 (delta 17), pack-reused 390\u001b[K\n",
            "Receiving objects: 100% (455/455), 75.37 KiB | 2.79 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import pinecone\n",
        "\n",
        "\n",
        "# Add Vars here \n",
        "\n",
        "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
        "# find environment next to your API key in the Pinecone console\n",
        "env = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\n",
        "\n",
        "index_name = \"langchain-pinecone-hybrid-search\"\n",
        "\n",
        "pinecone.init(api_key=api_key, enviroment=env)\n",
        "pinecone.whoami()"
      ],
      "metadata": {
        "id": "9iIacDfgpB2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16391aa9-c548-4314-c785-7745f2fc0f18"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WhoAmIResponse(username='9eb4a41', user_label='default', projectname='1780fef')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pinecone"
      ],
      "metadata": {
        "id": "mqpu7wYzp97c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index Setup"
      ],
      "metadata": {
        "id": "mcYYrFEOqAFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # create the index\n",
        "pinecone.create_index(\n",
        "   name = index_name,\n",
        "   dimension = 1536,  # dimensionality of dense model\n",
        "   metric = \"dotproduct\",  # sparse values supported only for dotproduct\n",
        "   pod_type = \"s1\",\n",
        "   metadata_config={\"indexed\": []}  # see explaination above\n",
        ")"
      ],
      "metadata": {
        "id": "R8t1H2QGo6Dg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pinecone.Index(index_name)"
      ],
      "metadata": {
        "id": "ORO6M3s1ps8R"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Sparse Embeddings"
      ],
      "metadata": {
        "id": "H-CsftTlqCcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone_text.sparse import BM25Encoder\n",
        "# or from pinecone_text.sparse import SpladeEncoder if you wish to work with SPLADE\n",
        "\n",
        "# use default tf-idf values\n",
        "bm25_encoder = BM25Encoder().default()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Scrmnl4xqGBq",
        "outputId": "c9adc219-8c59-4ce0-e358-8b2ef157a518"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"foo\", \"bar\", \"world\", \"hello\"]\n",
        "\n",
        "# fit tf-idf values on your corpus\n",
        "bm25_encoder.fit(corpus)\n",
        "\n",
        "# store the values to a json file\n",
        "bm25_encoder.dump(\"bm25_values.json\")\n",
        "\n",
        "# load to your BM25Encoder object\n",
        "bm25_encoder = BM25Encoder().load(\"bm25_values.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6c077038dfcc4d67bb70d2162dd2d339",
            "8c911fac374947238de9f647a2e9244f",
            "acb817ca80ec45fda84bd0ca70ca5e68",
            "8bd874f7cb2e45d9a2bb7e25642cb46b",
            "8055bc7170314316a2d1bedf82595660",
            "d18a5ea443a64825b4367ff38f96d442",
            "a38c4a01a17e4f36a02fe3f679b53ab3",
            "924dcaf797c8442f9d582428893b738d",
            "be66921235fe48f7a245837b4fc1652b",
            "4eb32f9e9ad04b9da93445d864bb9e01",
            "99511ff3cee047018fc1b2dd7f7921d3"
          ]
        },
        "id": "FTR_cRKKrJvz",
        "outputId": "df58231d-c440-4c8b-8d37-16d8d93d10dc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c077038dfcc4d67bb70d2162dd2d339"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "MaOOsob7rOur",
        "outputId": "f1b8ec2f-3cf0-4202-f69b-0b8a2bfda0e6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-807b0c6d43a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPineconeHybridSearchRetriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_encoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbm25_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'PineconeHybridSearchRetriever' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Imports"
      ],
      "metadata": {
        "id": "D-xhK2XHdjEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import PALChain, OpenAPIEndpointChain\n",
        "from langchain.prompts import PromptTemplate, Prompt,AIMessagePromptTemplate\n",
        "from langchain.llms import OpenAI, LlamaCpp, HuggingFaceHub, HuggingFaceEndpoint, OpenAIChat\n",
        "from langchain.chains import LLMChain, TransformChain, AnalyzeDocumentChain, APIChain\n",
        "from langchain import OpenAI, LLMChain\n",
        "from langchain.retrievers import PineconeHybridSearchRetriever"
      ],
      "metadata": {
        "id": "qHGIsDdIbpYf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "X4DgN4kNdtPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "chat_llm = ChatOpenAI(model_name='gpt-3.5-turbo', \n",
        "             temperature=0, \n",
        "             max_tokens=1024)"
      ],
      "metadata": {
        "id": "XXyKSQt0dv_h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oai_llm = OpenAI(model_name='text-davinci-003', \n",
        "             temperature=0, \n",
        "             max_tokens = 256)"
      ],
      "metadata": {
        "id": "m6zQCmqEd1jw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains"
      ],
      "metadata": {
        "id": "hJC0Hbsvd_LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain, SequentialChain, APIChain\n",
        "from langchain.chains.api import open_meteo_docs, podcast_docs"
      ],
      "metadata": {
        "id": "Vz8zETHjeAmV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts"
      ],
      "metadata": {
        "id": "rdb4p6hIeV0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.api.prompt import API_RESPONSE_PROMPT\n",
        "from langchain.prompts.prompt import PromptTemplate"
      ],
      "metadata": {
        "id": "XVY1DfRoeYA0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings "
      ],
      "metadata": {
        "id": "Ff0OqMP4pw64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "ovBtqChvpy5R"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import PineconeHybridSearchRetriever\n",
        "retriever = PineconeHybridSearchRetriever(embeddings=embeddings, sparse_encoder=bm25_encoder, index=index)"
      ],
      "metadata": {
        "id": "JAWO9yIlrqrh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.add_texts([\"foo\", \"bar\", \"world\", \"hello\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "dad8dc686d0b4a70b296290bcee2c11c",
            "496df0f2c59648688af76c81b1e08594",
            "04dc5a4e611843f6bf381546d168beea",
            "57ec014c3f4c4cb18e919d8d10dc64ef",
            "73524d19561843b1a26ed9cdabb7068a",
            "69f1d6da3184479681af47b70ef48cfe",
            "f897de046c8d4dda808df2f7f437df96",
            "915cd7a2be394c16b661e9652b7e55a9",
            "e40d00a5b3cf4d7d86f5f78cd11aa73d",
            "f753e3b43db44986bb344bd5eadce65a",
            "90290db43cf84dca84fb348d6c7fb213"
          ]
        },
        "id": "FKQzJaagsDSu",
        "outputId": "97734c66-b046-4904-c081-cfef0c4ef680"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dad8dc686d0b4a70b296290bcee2c11c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = retriever.get_relevant_documents(\"foo\")"
      ],
      "metadata": {
        "id": "EAM5GfmlsTda"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-1j0l7lsWER",
        "outputId": "94f7c8d2-3911-45f1-c01b-a6c316cf4dde"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='foo', metadata={})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UQjFgd9nsV7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "meTtgw51epoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory()\n",
        "from langchain.memory.chat_memory import ChatMessageHistory\n",
        "from langchain.memory.chat_message_histories import RedisChatMessageHistory"
      ],
      "metadata": {
        "id": "f-3qnmrEescu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Message History"
      ],
      "metadata": {
        "id": "5nUyeYnCsnG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "history.add_user_message(\"hi!\")\n",
        "\n",
        "history.add_ai_message(\"whats up?\")"
      ],
      "metadata": {
        "id": "ICQlEVu7smL0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTJdPN64swt4",
        "outputId": "d54d0f44-5db1-4c2b-9227-1a9d66e377b8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='hi!', additional_kwargs={}),\n",
              " AIMessage(content='whats up?', additional_kwargs={})]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ConversationBufferMemory"
      ],
      "metadata": {
        "id": "u4YhKAsesxzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory()\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"whats up?\")"
      ],
      "metadata": {
        "id": "fHNjpOMRs1gE"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKHxkVM0s4Ij",
        "outputId": "a497f596-1722-4dfe-c7c4-0625779cf5b4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: hi!\\nAI: whats up?'}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"whats up?\")"
      ],
      "metadata": {
        "id": "YL0Ocryrs71M"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-Hh3d7Cs83F",
        "outputId": "b234892a-2e4b-48e6-d235-91b6a59f9f68"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='hi!', additional_kwargs={}),\n",
              "  AIMessage(content='whats up?', additional_kwargs={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    verbose=True, \n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ],
      "metadata": {
        "id": "xZNiEOnwtC06"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Hi there!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "ermfty2ItN3y",
        "outputId": "1d603de4-36ab-4cec-c1b7-927c02556b1e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi there! It's nice to meet you. My name is AI. What's your name?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "-o7sD0wFtU_u",
        "outputId": "2d8548de-a184-4de3-b956-6a65fed07fda"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Message History"
      ],
      "metadata": {
        "id": "cZ80qxrvtbvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain.schema import messages_from_dict, messages_to_dict\n",
        "\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "history.add_user_message(\"hi!\")\n",
        "\n",
        "history.add_ai_message(\"whats up?\")"
      ],
      "metadata": {
        "id": "J6eZYFgitd19"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dicts = messages_to_dict(history.messages)"
      ],
      "metadata": {
        "id": "q1HmRzaTtgL9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dicts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkShkv6NtiGx",
        "outputId": "35b86843-a30f-438d-a676-51fd560c8eb0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'type': 'human', 'data': {'content': 'hi!', 'additional_kwargs': {}}},\n",
              " {'type': 'ai', 'data': {'content': 'whats up?', 'additional_kwargs': {}}}]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_messages = messages_from_dict(dicts)"
      ],
      "metadata": {
        "id": "MkAIbJd5tk6d"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67RsZqQ5tmrB",
        "outputId": "2c243abb-447c-4130-9f7d-731f8bfa2e1b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='hi!', additional_kwargs={}),\n",
              " AIMessage(content='whats up?', additional_kwargs={})]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entity History"
      ],
      "metadata": {
        "id": "ru0_8r-quToL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationEntityMemory\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "Hs7zDWlYuV3s"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationEntityMemory(llm=llm)\n",
        "_input = {\"input\": \"Dave is working on a Grim Dark Future Game app\"}\n",
        "memory.load_memory_variables(_input)\n",
        "memory.save_context(\n",
        "    _input,\n",
        "    {\"ouput\": \" Grim Dark Future is a tabletop game based on Warhammer 40K.  The rules are designed to be flexible and modular, so players can customize their games and add their own unique rules and scenarios. \"}\n",
        ")"
      ],
      "metadata": {
        "id": "vnCSWobXuXoq"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({\"input\": 'What is Grim Dark Future?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CZjfPHQvEZU",
        "outputId": "2cb22a3a-8f5f-4953-d3b6-bbd931aba368"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Dave is working on a Grim Dark Future Game app\\nAI:  Grim Dark Future is a tabletop game based on Warhammer 40K.  The rules are designed to be flexible and modular, so players can customize their games and add their own unique rules and scenarios. ',\n",
              " 'entities': {'Grim Dark Future': ''}}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using in a Chain"
      ],
      "metadata": {
        "id": "7TTqu9A-vTbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationEntityMemory\n",
        "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict, Any"
      ],
      "metadata": {
        "id": "uE4a1bgqvVkC"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        "    verbose=True,\n",
        "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
        "    memory=ConversationEntityMemory(llm=llm)\n",
        ")"
      ],
      "metadata": {
        "id": "pFvz0anHvYT-"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What are the rules of Grim Dark Future?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "N8PZuFeIvZdb",
        "outputId": "0d0b0079-6c8d-4784-99e4-0a31fe2591a5"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Grim Dark Future': ''}\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Last line:\n",
            "Human: What are the rules of Grim Dark Future?\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Grim Dark Future is a post-apocalyptic setting where the world has been ravaged by war and chaos. The rules of the game are designed to create a gritty and dangerous atmosphere, where players must fight for survival and resources. The game focuses on scavenging, crafting, and combat, with a variety of weapons and equipment available to players. Additionally, the game features a unique morality system, where players must make difficult decisions that can have lasting consequences.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.memory.entity_store.store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBD_Z_PXvutt",
        "outputId": "a9802eac-711e-485e-dbdc-a906ef7baff6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Dave': 'Dave is working on a Grim Dark Future Game app.',\n",
              " 'Grim Dark Future Game': 'Grim Dark Future Game is a tabletop game based on Warhammer 40K, with customizable rules and scenarios, and Dave is currently working on an app for it.',\n",
              " 'Grim Dark Future': 'Grim Dark Future is a post-apocalyptic setting where the world has been ravaged by war and chaos, featuring scavenging, crafting, and combat with a variety of weapons and equipment, as well as a unique morality system with lasting consequences.'}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I am trying to build an army in Grim Dark Future.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "IVtMsm6Kvzbl",
        "outputId": "bea9a40b-40f2-44e3-a0d3-f9c1738b6687"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Grim Dark Future': 'Grim Dark Future is a post-apocalyptic setting where the world has been ravaged by war and chaos, featuring scavenging, crafting, and combat with a variety of weapons and equipment, as well as a unique morality system with lasting consequences.'}\n",
            "\n",
            "Current conversation:\n",
            "Human: What are the rules of Grim Dark Future?\n",
            "AI:  Grim Dark Future is a post-apocalyptic setting where the world has been ravaged by war and chaos. The rules of the game are designed to create a gritty and dangerous atmosphere, where players must fight for survival and resources. The game focuses on scavenging, crafting, and combat, with a variety of weapons and equipment available to players. Additionally, the game features a unique morality system, where players must make difficult decisions that can have lasting consequences.\n",
            "Last line:\n",
            "Human: I am trying to build an army in Grim Dark Future.\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Building an army in Grim Dark Future requires careful planning and strategy. You will need to consider the resources available to you, such as weapons, equipment, and supplies, as well as the type of army you want to build. You will also need to consider the terrain and environment you will be fighting in, as well as the type of enemies you will be facing. Finally, you will need to decide on a strategy for how to deploy your forces and how to best use them in battle.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "pprint(conversation.memory.entity_store.store)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypg7Urm_wGAW",
        "outputId": "9a4bad63-2277-46af-898c-0c9cc1098fcf"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Dave': 'Dave is working on a Grim Dark Future Game app.',\n",
            " 'Grim Dark Future': 'Grim Dark Future is a post-apocalyptic setting where the '\n",
            "                     'world has been ravaged by war and chaos, featuring '\n",
            "                     'scavenging, crafting, and combat with a variety of '\n",
            "                     'weapons and equipment, as well as a unique morality '\n",
            "                     'system with lasting consequences. Building an army in '\n",
            "                     'Grim Dark Future requires careful planning and strategy, '\n",
            "                     'including consideration of resources, terrain, '\n",
            "                     'environment, and enemies.',\n",
            " 'Grim Dark Future Game': 'Grim Dark Future Game is a tabletop game based on '\n",
            "                          'Warhammer 40K, with customizable rules and '\n",
            "                          'scenarios, and Dave is currently working on an app '\n",
            "                          'for it.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What do you know about Grim Dark Future?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "4uJPwqm6wN_Q",
        "outputId": "43151b0e-d2d9-4c3b-8803-b7f5a00ed406"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
            "\n",
            "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
            "\n",
            "Context:\n",
            "{'Grim Dark Future': 'Grim Dark Future is a post-apocalyptic setting where the world has been ravaged by war and chaos, featuring scavenging, crafting, and combat with a variety of weapons and equipment, as well as a unique morality system with lasting consequences. Building an army in Grim Dark Future requires careful planning and strategy, including consideration of resources, terrain, environment, and enemies.'}\n",
            "\n",
            "Current conversation:\n",
            "Human: What are the rules of Grim Dark Future?\n",
            "AI:  Grim Dark Future is a post-apocalyptic setting where the world has been ravaged by war and chaos. The rules of the game are designed to create a gritty and dangerous atmosphere, where players must fight for survival and resources. The game focuses on scavenging, crafting, and combat, with a variety of weapons and equipment available to players. Additionally, the game features a unique morality system, where players must make difficult decisions that can have lasting consequences.\n",
            "Human: I am trying to build an army in Grim Dark Future.\n",
            "AI:  Building an army in Grim Dark Future requires careful planning and strategy. You will need to consider the resources available to you, such as weapons, equipment, and supplies, as well as the type of army you want to build. You will also need to consider the terrain and environment you will be fighting in, as well as the type of enemies you will be facing. Finally, you will need to decide on a strategy for how to deploy your forces and how to best use them in battle.\n",
            "Last line:\n",
            "Human: What do you know about Grim Dark Future?\n",
            "You:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Grim Dark Future is a post-apocalyptic setting where the world has been ravaged by war and chaos. It features scavenging, crafting, and combat with a variety of weapons and equipment, as well as a unique morality system with lasting consequences. Building an army in Grim Dark Future requires careful planning and strategy, including consideration of resources, terrain, environment, and enemies. Additionally, the game features a variety of characters and factions, each with their own motivations and goals.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "0ljcaczbewv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor"
      ],
      "metadata": {
        "id": "-sL_0Y6bey7o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schema"
      ],
      "metadata": {
        "id": "zRyY3XL_e0Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import messages_from_dict, messages_to_dict"
      ],
      "metadata": {
        "id": "HNKDiaxRe2Nd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "rGeNkZ5ofAzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utilities import GoogleSearchAPIWrapper"
      ],
      "metadata": {
        "id": "TcsDSDehfKA3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "human_message_prompt = HumanMessagePromptTemplate(\n",
        "        prompt=PromptTemplate(\n",
        "            template=\"What is a good name for a company that makes {product}?\",\n",
        "            input_variables=[\"product\"],\n",
        "        )\n",
        "    )\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "chat = ChatOpenAI(temperature=0.9)\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
        "print(chain.run(\"tabletop games\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCEmr7NEgRcw",
        "outputId": "09e6125b-1e00-4b2e-b46a-33e2a6ca78a1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GameTopia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Hub"
      ],
      "metadata": {
        "id": "NRKTyrN1hAtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import load_chain\n",
        "\n",
        "llm_requests = load_chain('/content/langchain-hub/chains/llm-requests/chain.json')\n",
        "llm_math = load_chain('/content/langchain-hub/chains/llm-math/chain.json')\n",
        "llm_bash = load_chain('/content/langchain-hub/chains/llm-bash/chain.json')"
      ],
      "metadata": {
        "id": "YaSoa8zChvOX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "llm = ...\n",
        "tools = ...\n",
        "\n",
        "agent = initialize_agent(tools, llm, agent=\"/content/langchain-hub/agents/self-ask-with-search/agent.json\")"
      ],
      "metadata": {
        "id": "cDtBGzb_hDnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic LLMChain - Fact Extraction"
      ],
      "metadata": {
        "id": "-KB9qA8bpxgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTiEn3tKp7mZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fact_extraction_prompt = PromptTemplate(\n",
        "#     input_variables=[\"text_input\"],\n",
        "#     template=\"Extract the key facts out of this text. Don't include opinions. Give each fact a number and keep them short sentences. :\\n\\n {text_input}\"\n",
        "# )\n",
        "# fact_extraction_chain = LLMChain(llm=llm, prompt=fact_extraction_prompt)\n",
        "\n",
        "# facts = fact_extraction_chain.run(article)\n",
        "\n",
        "# print(facts)\n"
      ],
      "metadata": {
        "id": "qYPq1mrSfVTt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rewrite as a summary from the facts"
      ],
      "metadata": {
        "id": "YR2JtDm0hs03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template_content = \"\"\"\n",
        "You are a Goldman Sachs analyst.\n",
        "Take the following list of facts and use them to write a short paragrah for investors.\n",
        " Don't leave out key info:\n",
        " \n",
        "{facts}\n",
        "\"\"\"\n",
        "investor_update_prompt = PromptTemplate(\n",
        "    input_variables=[\"facts\"],\n",
        "    template=template_content\n",
        ")"
      ],
      "metadata": {
        "id": "nG--CuMJh1Gi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# investor_update_chain = LLMChain(llm=llm, prompt=investor_update_prompt)\n",
        "\n",
        "# investor_update = investor_update_chain.run(facts)\n",
        "\n",
        "# print(investor_update)\n",
        "# len(investor_update)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48761398-cd5c-4a57-9a22-a0fb93585d95",
        "id": "1rJu6HOoh1Gi"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the start of 2022.\n",
            "\n",
            "Coinbase released its Q4 2022 earnings on Tuesday, reporting total revenue of $605 million, which was slightly above Wall Street's expectations of $581.2 million. Despite the positive revenue figure, Coinbase reported a GAAP loss of $557 million in the three-month period. Coinbase's stock had risen 86% year-to-date before its Q4 earnings were released. Consumer trading volumes fell from $26 billion in Q3 2022 to $20 billion in Q4 2022, while institutional volumes across the same timeframe fell from $133 billion to $125 billion. The overall crypto market capitalization fell about 64%, or $1.5 trillion during 2022, leading to a 50% and 66% year-over-year decline in Coinbase's total trading volumes and transaction revenues, respectively. Trading revenue at Coinbase fell from $365.9 million in Q3 2022 to $322.1 million in Q4 2022, while its \"subscription and services revenue\" rose from $210.5 million in Q3 2022 to $282.8 million in Q4 2022. Despite the overall decline in the crypto market, monthly active developers in crypto have more than doubled since the start of 2022.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1105"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uCEfXHyFi-K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# triples_prompt = PromptTemplate(\n",
        "#     input_variables=[\"facts\"],\n",
        "#     template=\"Take the following list of facts and turn them into triples for a knowledge graph:\\n\\n {facts}\"\n",
        "# )"
      ],
      "metadata": {
        "id": "mQ2TNW1emNHu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# triples_chain = LLMChain(llm=llm, prompt=triples_prompt)\n",
        "\n",
        "# triples = triples_chain.run(facts)\n",
        "\n",
        "# print(triples)\n",
        "# len(triples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0f5689-091d-4a19-96f8-fac8b7e4f721",
        "id": "ZWyFFkDomNH5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the start of 2022.\n",
            "\n",
            "1. (Coinbase, released, Q4 2022 earnings)\n",
            "2. (Coinbase, generated, $605 million total revenue)\n",
            "3. (Coinbase, lost, $557 million GAAP basis)\n",
            "4. (Wall Street, expected, $581.2 million revenue)\n",
            "5. (Coinbase's stock, risen, 86% year-to-date)\n",
            "6. (Consumer trading volumes, fell, $26 billion Q3 2022)\n",
            "7. (Institutional volumes, fell, $133 billion)\n",
            "8. (Overall crypto market capitalization, fell, $1.5 trillion)\n",
            "9. (Coinbase's total trading volumes, fell, 50% year-over-year)\n",
            "10. (Coinbase's transaction revenues, fell, 66% year-over-year)\n",
            "11. (Trading revenue, fell, $365.9 million Q3 2022)\n",
            "12. (Coinbase's subscription and services revenue, rose, $210.5 million Q3 2022)\n",
            "13. (Monthly active developers in crypto, more than doubled, since start of 2022)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chaining these together"
      ],
      "metadata": {
        "id": "-FNzbLpWnRy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# full_chain = SimpleSequentialChain(chains=[fact_extraction_chain, investor_update_chain], verbose=True)"
      ],
      "metadata": {
        "id": "XZLtU9O-nVya"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# response = full_chain.run(article)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fomCgIYCnsQA",
        "outputId": "26748966-8236-45f6-bb58-6723db8cc5ec"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "1. Coinbase released its Q4 2022 earnings on Tuesday.\n",
            "2. Coinbase generated $605 million in total revenue in Q4 2022.\n",
            "3. Coinbase lost $557 million in the three-month period on a GAAP basis.\n",
            "4. Wall Street expected Coinbase to report $581.2 million in revenue.\n",
            "5. Coinbase's stock had risen 86% year-to-date before its Q4 earnings were released.\n",
            "6. Consumer trading volumes fell from $26 billion in Q3 2022 to $20 billion in Q4 2022.\n",
            "7. Institutional volumes across the same timeframe fell from $133 billion to $125 billion.\n",
            "8. The overall crypto market capitalization fell about 64%, or $1.5 trillion during 2022.\n",
            "9. Coinbase's total trading volumes and transaction revenues fell 50% and 66% year-over-year, respectively.\n",
            "10. Trading revenue at Coinbase fell from $365.9 million in Q3 2022 to $322.1 million in Q4 2022.\n",
            "11. Coinbase's \"subscription and services revenue\" rose from $210.5 million in Q3 2022 to $282.8 million in Q4 2022.\n",
            "12. Monthly active developers in crypto have more than doubled since\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m the start of 2022.\n",
            "\n",
            "Coinbase released its Q4 2022 earnings on Tuesday, reporting total revenue of $605 million, which was slightly above Wall Street's expectations of $581.2 million. Despite the positive revenue figure, Coinbase reported a GAAP loss of $557 million in the three-month period. Coinbase's stock had risen 86% year-to-date before its Q4 earnings were released. Consumer trading volumes fell from $26 billion in Q3 2022 to $20 billion in Q4 2022, while institutional volumes across the same timeframe fell from $133 billion to $125 billion. The overall crypto market capitalization fell about 64%, or $1.5 trillion during 2022, leading to a 50% and 66% year-over-year decline in Coinbase's total trading volumes and transaction revenues, respectively. Trading revenue at Coinbase fell from $365.9 million in Q3 2022 to $322.1 million in Q4 2022, while its \"subscription and services revenue\" rose from $210.5 million in Q3 2022 to $282.8 million in Q4 2022. Despite the overall decline in the crypto market, monthly active developers in crypto have more than doubled since the start of 2022.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PAL Math Chain"
      ],
      "metadata": {
        "id": "2l3HG0YUqLlF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZF-jpK_XqPK0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ggg2goAsqX2F"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pal_chain = PALChain.from_math_prompt(llm, verbose=True)"
      ],
      "metadata": {
        "id": "d7QPImg8qX9F"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Jan has three times the number of pets as Marcia. Marcia has two more pets than Cindy. If Cindy has four pets, how many total pets do the three have?\""
      ],
      "metadata": {
        "id": "bqHtkj5wqYCZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_02= \"The cafeteria had 23 apples. If they used 20 for lunch and bought 6 more, how many apples do they have?\""
      ],
      "metadata": {
        "id": "2NMqJo2zrYdf"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pal_chain.run(question_02)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "ZOlbkO5Kqv4k",
        "outputId": "c39f8d73-4575-4c01-c78b-2c27e680a118"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new PALChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mdef solution():\n",
            "    \"\"\"The cafeteria had 23 apples. If they used 20 for lunch and bought 6 more, how many apples do they have?\"\"\"\n",
            "    apples_initial = 23\n",
            "    apples_used = 20\n",
            "    apples_bought = 6\n",
            "    apples_left = apples_initial - apples_used + apples_bought\n",
            "    result = apples_left\n",
            "    return result\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'9'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Chains - OpenMeteo - Weather information\n",
        "\n",
        "can throw errors based on API return length"
      ],
      "metadata": {
        "id": "xidOhWp7rk_5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8p96niVlRPh8"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0,\n",
        "             max_tokens=100)"
      ],
      "metadata": {
        "id": "BsPzhnj_RUPy"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chain_new = APIChain.from_llm_and_api_docs(llm, \n",
        "                                           open_meteo_docs.OPEN_METEO_DOCS, \n",
        "                                           verbose=True)"
      ],
      "metadata": {
        "id": "THJPYYB_cD8U"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_new.run('What is the temperature like right now in Bedok, Singapore in degrees Celcius?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "mkfkkjpzcKym",
        "outputId": "20e94c9d-e89a-4a91-c2f5-697062924b12"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mhttps://api.open-meteo.com/v1/forecast?latitude=1.3&longitude=103.9&hourly=temperature_2m&current_weather=true&temperature_unit=celsius\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m{\"latitude\":1.375,\"longitude\":103.875,\"generationtime_ms\":0.15091896057128906,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":6.0,\"current_weather\":{\"temperature\":30.7,\"windspeed\":8.4,\"winddirection\":205.0,\"weathercode\":3,\"is_day\":1,\"time\":\"2023-04-21T05:00\"},\"hourly_units\":{\"time\":\"iso8601\",\"temperature_2m\":\"°C\"},\"hourly\":{\"time\":[\"2023-04-21T00:00\",\"2023-04-21T01:00\",\"2023-04-21T02:00\",\"2023-04-21T03:00\",\"2023-04-21T04:00\",\"2023-04-21T05:00\",\"2023-04-21T06:00\",\"2023-04-21T07:00\",\"2023-04-21T08:00\",\"2023-04-21T09:00\",\"2023-04-21T10:00\",\"2023-04-21T11:00\",\"2023-04-21T12:00\",\"2023-04-21T13:00\",\"2023-04-21T14:00\",\"2023-04-21T15:00\",\"2023-04-21T16:00\",\"2023-04-21T17:00\",\"2023-04-21T18:00\",\"2023-04-21T19:00\",\"2023-04-21T20:00\",\"2023-04-21T21:00\",\"2023-04-21T22:00\",\"2023-04-21T23:00\",\"2023-04-22T00:00\",\"2023-04-22T01:00\",\"2023-04-22T02:00\",\"2023-04-22T03:00\",\"2023-04-22T04:00\",\"2023-04-22T05:00\",\"2023-04-22T06:00\",\"2023-04-22T07:00\",\"2023-04-22T08:00\",\"2023-04-22T09:00\",\"2023-04-22T10:00\",\"2023-04-22T11:00\",\"2023-04-22T12:00\",\"2023-04-22T13:00\",\"2023-04-22T14:00\",\"2023-04-22T15:00\",\"2023-04-22T16:00\",\"2023-04-22T17:00\",\"2023-04-22T18:00\",\"2023-04-22T19:00\",\"2023-04-22T20:00\",\"2023-04-22T21:00\",\"2023-04-22T22:00\",\"2023-04-22T23:00\",\"2023-04-23T00:00\",\"2023-04-23T01:00\",\"2023-04-23T02:00\",\"2023-04-23T03:00\",\"2023-04-23T04:00\",\"2023-04-23T05:00\",\"2023-04-23T06:00\",\"2023-04-23T07:00\",\"2023-04-23T08:00\",\"2023-04-23T09:00\",\"2023-04-23T10:00\",\"2023-04-23T11:00\",\"2023-04-23T12:00\",\"2023-04-23T13:00\",\"2023-04-23T14:00\",\"2023-04-23T15:00\",\"2023-04-23T16:00\",\"2023-04-23T17:00\",\"2023-04-23T18:00\",\"2023-04-23T19:00\",\"2023-04-23T20:00\",\"2023-04-23T21:00\",\"2023-04-23T22:00\",\"2023-04-23T23:00\",\"2023-04-24T00:00\",\"2023-04-24T01:00\",\"2023-04-24T02:00\",\"2023-04-24T03:00\",\"2023-04-24T04:00\",\"2023-04-24T05:00\",\"2023-04-24T06:00\",\"2023-04-24T07:00\",\"2023-04-24T08:00\",\"2023-04-24T09:00\",\"2023-04-24T10:00\",\"2023-04-24T11:00\",\"2023-04-24T12:00\",\"2023-04-24T13:00\",\"2023-04-24T14:00\",\"2023-04-24T15:00\",\"2023-04-24T16:00\",\"2023-04-24T17:00\",\"2023-04-24T18:00\",\"2023-04-24T19:00\",\"2023-04-24T20:00\",\"2023-04-24T21:00\",\"2023-04-24T22:00\",\"2023-04-24T23:00\",\"2023-04-25T00:00\",\"2023-04-25T01:00\",\"2023-04-25T02:00\",\"2023-04-25T03:00\",\"2023-04-25T04:00\",\"2023-04-25T05:00\",\"2023-04-25T06:00\",\"2023-04-25T07:00\",\"2023-04-25T08:00\",\"2023-04-25T09:00\",\"2023-04-25T10:00\",\"2023-04-25T11:00\",\"2023-04-25T12:00\",\"2023-04-25T13:00\",\"2023-04-25T14:00\",\"2023-04-25T15:00\",\"2023-04-25T16:00\",\"2023-04-25T17:00\",\"2023-04-25T18:00\",\"2023-04-25T19:00\",\"2023-04-25T20:00\",\"2023-04-25T21:00\",\"2023-04-25T22:00\",\"2023-04-25T23:00\",\"2023-04-26T00:00\",\"2023-04-26T01:00\",\"2023-04-26T02:00\",\"2023-04-26T03:00\",\"2023-04-26T04:00\",\"2023-04-26T05:00\",\"2023-04-26T06:00\",\"2023-04-26T07:00\",\"2023-04-26T08:00\",\"2023-04-26T09:00\",\"2023-04-26T10:00\",\"2023-04-26T11:00\",\"2023-04-26T12:00\",\"2023-04-26T13:00\",\"2023-04-26T14:00\",\"2023-04-26T15:00\",\"2023-04-26T16:00\",\"2023-04-26T17:00\",\"2023-04-26T18:00\",\"2023-04-26T19:00\",\"2023-04-26T20:00\",\"2023-04-26T21:00\",\"2023-04-26T22:00\",\"2023-04-26T23:00\",\"2023-04-27T00:00\",\"2023-04-27T01:00\",\"2023-04-27T02:00\",\"2023-04-27T03:00\",\"2023-04-27T04:00\",\"2023-04-27T05:00\",\"2023-04-27T06:00\",\"2023-04-27T07:00\",\"2023-04-27T08:00\",\"2023-04-27T09:00\",\"2023-04-27T10:00\",\"2023-04-27T11:00\",\"2023-04-27T12:00\",\"2023-04-27T13:00\",\"2023-04-27T14:00\",\"2023-04-27T15:00\",\"2023-04-27T16:00\",\"2023-04-27T17:00\",\"2023-04-27T18:00\",\"2023-04-27T19:00\",\"2023-04-27T20:00\",\"2023-04-27T21:00\",\"2023-04-27T22:00\",\"2023-04-27T23:00\"],\"temperature_2m\":[27.1,28.0,28.8,29.6,30.6,30.7,31.1,30.8,30.8,30.5,30.0,29.5,28.9,28.5,28.2,28.0,27.8,27.5,27.2,27.1,26.9,26.8,26.7,26.6,27.0,27.9,28.9,29.5,29.9,30.4,30.1,29.8,30.1,29.9,29.5,29.0,28.5,28.0,27.9,27.7,27.5,27.4,27.3,27.1,27.0,26.9,26.9,26.9,27.4,28.5,29.5,30.4,31.1,30.9,31.1,31.4,30.7,29.8,29.5,29.0,28.3,27.8,27.5,27.3,27.1,26.9,26.7,26.5,26.5,26.4,26.3,26.1,26.6,28.1,29.6,30.8,32.1,33.1,33.4,33.2,32.4,31.4,30.7,30.1,29.2,28.6,28.1,27.5,27.1,26.8,26.5,26.3,26.0,25.9,25.8,25.9,26.4,27.5,29.0,30.7,31.4,31.9,32.0,31.3,30.3,29.0,28.5,28.3,28.0,27.8,27.7,27.6,27.4,27.2,27.0,26.8,26.6,26.5,26.4,26.4,26.8,27.7,28.9,30.3,30.8,31.2,31.3,31.1,30.6,29.9,29.4,29.0,28.4,28.2,28.0,27.7,27.5,27.2,26.8,26.6,26.4,26.2,26.0,26.0,26.4,27.5,29.0,30.7,31.4,31.9,32.2,32.1,31.8,31.3,30.7,29.9,29.1,28.6,28.3,27.9,27.7,27.4,27.2,27.0,26.7,26.6,26.5,26.4]}}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The temperature right now in Bedok, Singapore is 30.7 degrees Celcius.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_new.run('Is it raining in Singapore?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "0qziCnvURUkI",
        "outputId": "a7af19be-df86-4f2a-adfa-1e6f373ec598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mhttps://api.open-meteo.com/v1/forecast?latitude=1.3521&longitude=103.8198&current_weather=true&rain=true\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m{\"latitude\":1.375,\"longitude\":103.875,\"generationtime_ms\":0.41103363037109375,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":46.0,\"current_weather\":{\"temperature\":25.8,\"windspeed\":10.5,\"winddirection\":16.0,\"weathercode\":3,\"time\":\"2023-02-22T14:00\"}}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' It is currently 25.8°C in Singapore with a windspeed of 10.5 km/h and a winddirection of 16°. The weathercode is 3, which indicates that it is currently raining in Singapore.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zN6kl7u7eK1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}